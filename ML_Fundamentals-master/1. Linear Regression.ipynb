{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.2em;\n",
       "line-height:1.4em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: -0.4em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Arial';\n",
       "font-size:1.5em;\n",
       "line-height:1.4em;\n",
       "padding-left:3em;\n",
       "padding-right:3em;\n",
       "}\n",
       "\n",
       ".MathJax {\n",
       "    font-size: 70%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "css_file = './custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "© 2018 Daniel Voigt Godoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "\n",
    "A linear regression is a simple and straightforward model: it is a ***weighted sum*** of the feature values plus a constant. The constant $b$ is called ***bias*** (or intercept). Each of $n$ features, $x_1, x_2 \\dots x_n \\\\ $, has a corresponding ***weight*** (or coefficient) $w_1, w_2 \\dots w_n \\\\ $ associated with it.\n",
    "\n",
    "So, a linear regression model can be expressed like this:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b + w_1x_1 + w_2x_2 + \\dots + w_nx_n\n",
    "$$\n",
    "\n",
    "But you can organize all the ***weights*** (and the ***bias*** too), as well as the ***features***, into ***vectors*** like this:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} = \\begin{bmatrix}\n",
    "           b \\\\\n",
    "           w_{1} \\\\\n",
    "           w_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           w_{n}\n",
    "\\end{bmatrix} \\\n",
    "\\boldsymbol{x} = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then ***this*** becomes the ***vectorized*** version of the linear regression model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\boldsymbol{w}^T \\cdot \\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "### 1.1 Loss Function\n",
    "\n",
    "How do we ***train*** the model? If we were to ***randomly guess*** the ***weights***, we would very likely be wrong, but ***how*** wrong? We need a ***performance measure***!\n",
    "\n",
    "We can start by taking the ***difference*** between the predictions $\\hat{y} \\ $ and the actual responses $y \\ $ for each and every one of our $m \\ $ samples. We can also take the ***square*** of these differences, to penalize grossly wrong predictions. This is the ***Sum of Squared Errors (SSE)***:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{m}(y^{(i)}-\\hat{y}^{(i)})^2\n",
    "$$\n",
    "\n",
    "Moreover, it's much easier to have a ***single number*** instead of a whole bunch of squared differences, so we ***average*** those differences. Sounds like a plan, uh?\n",
    "\n",
    "There is actually a name for this: ***Mean Squared Error (MSE)***.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m}SSE = \\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-\\hat{y}^{(i)})^2 = \\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-\\boldsymbol{w}^T \\cdot \\boldsymbol{x}^{(i)})^2\n",
    "$$\n",
    "\n",
    "It turns out, minimizing ***MSE*** yields the best set of ***weights*** for a ***linear regression***.\n",
    "\n",
    "### 1.2 Minimizing the Loss Function\n",
    "\n",
    "Linear Regressions have a ***closed form solution***, that is, an equation that yields the results directly. Simple enough, right? \n",
    "\n",
    "There is a catch, though... its computing time grows to the power of 3.\n",
    "\n",
    "Two times more features? ***Eight times*** more time to compute the solution! \n",
    "\n",
    "If you have ***many*** features, it could be a problem... it does not look so good anymore...\n",
    "\n",
    "On the plus side, it is also possible to start with a ***random guess*** and work your way through the minimization, one step at a time, until you reach (well, not always!) your desired solution - the right values for ***bias*** and ***weights***. This process is called ***gradient descent*** and we'll look into it with more details in a future lesson.\n",
    "\n",
    "### 1.3 Goodness of Fit\n",
    "\n",
    "The ***coefficient of determination*** $R^2 \\ $ tells you how much of the variance in your response can be explained by your features. So, a ***r-squared value of 1*** means your model has a perfect fit - ***no errors*** in your predictions! \n",
    "\n",
    "On the other hand, a ***r-squared value of 0*** means your model is no better than a ***simple average*** over all points.\n",
    "\n",
    "Here is the formula:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SSE}{SS_{total}} = 1 - \\frac{\\sum_{i=1}^m(y^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^m(y^{(i)}-\\overline{y})^2}\n",
    "$$\n",
    "\n",
    "In a nutshell, it compares two kinds of differences:\n",
    "\n",
    "1. Actual responses vs ***your predictions***\n",
    "2. Actual responses vs ***average of actual responses***\n",
    "\n",
    "You can also think of this as comparing your predictions to the ***simplest possible prediction: the average***!\n",
    "\n",
    "![](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "<center>Source: <a href=\"https://xkcd.com/1725/\">XKCD</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment\n",
    "\n",
    "Time to try it yourself!\n",
    "\n",
    "Instead of using ***gradient descent***, we'll be using ***you*** to minimize the loss :-)\n",
    "\n",
    "You have a 100 data points with ***(x, y)*** coordinates.\n",
    "\n",
    "Like every traditional linear regression problem, ***x*** is your feature, ***y*** is your response, as in the equation below:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b + w_1x\n",
    "$$\n",
    "\n",
    "You want to start training your linear regression, so you need to find both the ***bias*** (intercept) $b$ and the ***single weight*** $w_1$ that ***minimize MSE*** (or SSE, for that matter).\n",
    "\n",
    "The sliders below allow you to change both values, and you can observe the effect they have on the distribution of errors, as well as your objective ***SSE*** and coefficient of determination $r^2$.\n",
    "\n",
    "Use the slider to play with different configurations and answer the ***questions*** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intuitiveml.supervised.regression.LinearRegression import *\n",
    "from intuitiveml.utils import gen_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fund/lib/python3.6/site-packages/plotly/tools.py:465: DeprecationWarning:\n",
      "\n",
      "plotly.tools.make_subplots is deprecated, please use plotly.subplots.make_subplots instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = data()\n",
    "mylr = plotLinearRegression(x, y)\n",
    "vb = VBox(build_figure(mylr))\n",
    "vb.layout.align_items = 'center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28a3ed8e6184f7eba47358bd30b9765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FigureWidget({\n",
       "    'data': [{'mode': 'markers',\n",
       "              'name': 'data',\n",
       "              'ty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "Plot on the right is the frequency distribution of errors. If the model is performing well, almost every error is close to zero. So the histogram is compressed around zero.\n",
    "Error: distance between \n",
    "\n",
    "\n",
    "1. What happens to the ***distribution of errors*** as you change the ***bias***?\n",
    "\n",
    "they move from 22.78 up until 3600, shifting on x\n",
    "\n",
    "2. What happens to the ***distribution of errors*** as you change the ***single weight***?\n",
    "\n",
    "they become a bit more normally distributed\n",
    "\n",
    "3. What is the combination of parameters that ***minimizes MSE(SSE)***?\n",
    "\n",
    "b: 0.80\n",
    "w: 3.30\n",
    "\n",
    "4. What happens to $r^2$ as you ***minimize MSE(SSE)***?\n",
    "\n",
    "moves closer to the value of 1\n",
    "\n",
    "5. What happens to $r^2$ if you set the ***weight equals zero*** and the ***bias equals mean***?\n",
    "\n",
    "it becomes -0\n",
    "\n",
    "6. Try getting a ***negative*** $r^2$. Does it happen? If so, under which conditions? What does it mean?\n",
    "\n",
    "by setting the weight to zero or lower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scikit-Learn\n",
    "\n",
    "[Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "Please check Aurelién Geron's \"Hand-On Machine Learning with Scikit-Learn and Tensorflow\" notebook on Linear Models [here](http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/04_training_linear_models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. More Resources\n",
    "\n",
    "[InfoGraphic - Simple Linear Regression](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg)\n",
    "\n",
    "[InfoGraphic - Multiple Linear Regression](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.jpg)\n",
    "\n",
    "[INTERACTIVE Ordinary Least Squares Regression](http://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "\n",
    "[INTERACTIVE Seeing Theory - Regression Analysis](https://seeing-theory.brown.edu/regression-analysis/index.html#section1)\n",
    "\n",
    "[Cofficient of Determination](https://towardsdatascience.com/coefficient-of-determination-r-squared-explained-db32700d924e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keras\n",
    "\n",
    "Keras is known for its user-friendly interface for implementing Deep Learning models with Tensorflow as backend.\n",
    "\n",
    "But you can also use it to train a simple linear regression like the one in the experiment above!\n",
    "\n",
    "This is probably the silliest \"neural network\" in history, though... it has ***ONE input*** $x$ and ***ONE output*** $y$, absolutely ***NO hidden units***, and it uses a ***linear*** activation function - it means it simply multiplies weights by feature values and nothing else! It also has a ***bias*** term.\n",
    "\n",
    "Effectively, it computes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b + w_1x\n",
    "$$\n",
    "\n",
    "We also tell Keras to use ***MSE*** as both ***loss*** and ***metric***.\n",
    "So far, it has all the same elements we already visited.\n",
    "\n",
    "Then there is the ***optimizer***: this is the piece responsible for tweaking the values of the ***bias*** and ***weight*** using ***gradient descent***. \n",
    "\n",
    "In the example, it is using ***Stochastic Gradient Descent*** (SGD) with a ***learning rate*** (lr) of 0.1.\n",
    "\n",
    "We'll see what that means in the another lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1, units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'], optimizer=SGD(lr=.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1, units=1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'], optimizer=SGD(lr=.1))\n",
    "\n",
    "model.fit(x, y, epochs=10, batch_size=16, verbose=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, epochs=10, batch_size=16, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.get_weights()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "1. The number on the right is the ***bias***, the one on the left, the ***weight***. Are they close to the ones you found while manually minimizing SSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This material is copyright Daniel Voigt Godoy and made available under the Creative Commons Attribution (CC-BY) license ([link](https://creativecommons.org/licenses/by/4.0/)). \n",
    "\n",
    "#### Code is also made available under the MIT License ([link](https://opensource.org/licenses/MIT))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
