{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 13:58:40.577782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-04 13:58:40.577807: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/stas/dsr/debugging-dl-models/ddlm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from utils.utils import make_writer\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common bugs II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Understanding LSTM networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Batch normalization explained](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "- [See-RNN package](https://github.com/OverLordGoldDragon/see-rnn)\n",
    "- [Gradient clipping](http://proceedings.mlr.press/v28/pascanu13.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical instabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of theory:\n",
    "\n",
    "- $X$ - input\n",
    "- $o$ - output\n",
    "- $L$ - number of layers in the network\n",
    "- $l$ - layer $l$ with the transormation $f_l$ and corresponding weights $W^l$ and the hidden variable $h^l$\n",
    "\n",
    " $$h^l = f_l(h^{l-1})$$ $$o = f_L \\circ ... \\circ f_1(X)$$\n",
    " \n",
    "If all $h^l$ and the input are vectors, one can write the gradient of $o$ with respect to any set of parameters $W^l$ as:\n",
    "\n",
    "$$\\partial_{W^l}o = \\partial_{h^{L-1}}{h^L} ... \\partial_{h^{l}}h^{l+1} \\partial_{W^{l}}h^l   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients:\n",
    "\n",
    "- Historically sigmoid was used as an activation function.\n",
    "- It resembles a thresholding function and was appealing, since neural nets were inspired by biological neural networks, where biological neurons either fire fully or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 13:58:55.379823: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-04 13:58:55.379851: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (certaintylab): /proc/driver/nvidia/version does not exist\n",
      "2022-03-04 13:58:55.380646: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tf.Variable(tf.range(-8.0, 8.0, 0.1))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.sigmoid(x)\n",
    "\n",
    "grad = tape.gradient(y, x).numpy()\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "    \n",
    "plt.plot(x, y, 'b', label='sigmoid')\n",
    "plt.plot(x, grad, 'r', label='gradient') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradients vanish for both big and small inputs.\n",
    "- Multiply many layers and if the inputs are not near zero, then a gradient can vanish.\n",
    "- Gradient cut off at some layer --> difficult to build deep nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- What's the standard solution for this specific instance of the problem?\n",
    "- Make a similar plot for the solution.\n",
    "- What is the problem of the standard solution? Can we improve it? Please, plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients in RNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Since we need to backpropagate through many time steps the gradients can vanish for long sequences.\n",
    " - Solution: use LSTM that has the cell state that plays a role of a skipped connection and helps the information to flow for the first time step to the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://drive.google.com/uc?id=1C01b5D2febgARxV8knVROaxoZMEynRcA\" width=\"600\" height=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding gradients and gradient clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.random.normal((4, 4))\n",
    "print(f'A single matrix \\n \\n {M.numpy()}')\n",
    "for i in range(100):\n",
    "    M = tf.matmul(M, tf.random.normal((4, 4)))\n",
    "\n",
    "print(f'\\nAfter multiplying 100 matrices \\n \\n {M.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient clipping\n",
    "\n",
    "- Clip a gradient by norm:\n",
    "    - $ \\textbf{g} \\gets min\\left(1, \\frac{\\theta}{||\\textbf{g}||}\\textbf{g} \\right)$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 6]$$ $$\\theta = 5$$ $$||\\textbf{g}|| = 7$$ $$\\textbf{g} \\gets [-2, 3, 6]\\cdot \\frac{5}{7}$$\n",
    "    \n",
    "- Clip gradient by value:\n",
    "    - If $g_i < \\theta_1$, then $g_i \\gets \\theta_1$ and $g_i > \\theta_2$, then $g_i \\gets \\theta_2$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 10]$$ $$\\theta_1 = 0, \\theta_2 = 5$$  $$ \\textbf{g} \\gets [0, 3, 5]$$\n",
    "\n",
    "    \n",
    "- Clip gradient by global norm:\n",
    "    - Rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold.\n",
    "    - For example: $$\\textbf{g}_1 = [-2, 3, 6]$$ $$\\textbf{g}_2= [-4, 6, 12]$$ $$\\theta = 14$$ $$||\\textbf{g}_1|| = 7$$ $$||\\textbf{g}_2|| = 14$$ $$\\textbf{g}_1 \\gets [-2, 3, 6]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}}$$ $$\\textbf{g}_2 \\gets [-4, 6, 12]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}} $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :].astype('float32'), X[n_train:, :].astype('float32')\n",
    "trainy, testy = y[:n_train].astype('float32'), y[n_train:].astype('float32')\n",
    "\n",
    "# Creat tf.Datasets \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy)).shuffle(trainX.shape[0]).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testX, testy)).shuffle(testX.shape[0]).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNet(tf.keras.Model):\n",
    "    def __init__(self, input_shape, optimizer):\n",
    "        super(RegressorNet, self).__init__()\n",
    "        \n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = optimizer\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(input_shape),\n",
    "            tf.keras.layers.Dense(25, activation='relu', kernel_initializer='he_uniform', name='dense_1'),\n",
    "            tf.keras.layers.Dense(1, activation='linear', name='out')\n",
    "        ])\n",
    "    \n",
    "    def summary(self):\n",
    "        self.regressor.summary()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.regressor(X)\n",
    "    \n",
    "    def get_loss(self, X, y_true):\n",
    "        y_pred = self(X)\n",
    "        l2_loss = self.loss_object(y_true, y_pred)\n",
    "        return l2_loss\n",
    "    \n",
    "    def grad_step(self, X, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(X, y_true)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an optimizer and an instance of the model\n",
    "optimizer = tf.keras.optimizers.SGD(0.01, 0.9)\n",
    "model_non_clipped = RegressorNet(input_shape=trainX.shape[1], optimizer=optimizer)\n",
    "# Show summary\n",
    "model_non_clipped.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss objects for calculation of the mean loss across batches\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataset, test_dataset, save_dir):\n",
    "    \n",
    "    writer = make_writer(os.path.join('summaries'), save_dir)\n",
    "    \n",
    "    for epoch in range(0, epochs + 1):\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {} is running...'.format(epoch))\n",
    "        \n",
    "        for X, y in train_dataset:\n",
    "            # Gradient update step\n",
    "            loss_train, gradients = model.grad_step(X, y)\n",
    "            train_loss(loss_train)\n",
    "            \n",
    "        for X, y in test_dataset:\n",
    "            # Gradient update step\n",
    "            loss_test = model.get_loss(X, y)\n",
    "            test_loss(loss_test)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Train loss: {train_loss.result()}')\n",
    "\n",
    "        # Tensorboard\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Test loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('Train loss', train_loss.result(), step=epoch)\n",
    "            \n",
    "            for layer_number, layer in enumerate(model.trainable_variables):\n",
    "                tf.summary.histogram(layer.name, gradients[layer_number], step=epoch, buckets=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_non_clipped, 100, train_dataset, test_dataset, 'exploding_grads/no_clipping/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Implement gradient clipping by norm OR by value OR by global norm in a new class RegressorNetClipped.\n",
    "- Plot gradients with clipping.\n",
    "\n",
    "A threshold is a parameter. In most of the cases it's a small number, usually around 1.\n",
    "However, one can experiment with that, bigger numbers speed up learning, but too big of a threshold can make it unstable.\n",
    "Another rule of thumb to choose a threshold is to monitor an average norm of the gradients for a big number of updates, then set the threshold to 5-10 times the value of that average.\n",
    "\n",
    "Hint: check `tf.clip_by_value`, `tf.clip_by_norm`, `tf.clip_by_global_norm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common issues and causes\n",
    "\n",
    "- Too big a tensor:\n",
    "    - Too large a batch size for your model \n",
    "    - Too many fully connected layers\n",
    "- Too much data:\n",
    "    - Loading a too big dataset into memory instead of using, e.g. tf.data queue loading\n",
    "    - Allocating to large a buffer for dataset creation\n",
    "- Duplicating operations:\n",
    "    - Memory leak due to creating multiple models at the same time\n",
    "    - Repeatedly creating an operation (e.g. in a function that gets called many times)\n",
    "- Other processes:\n",
    "    - Other processes taking GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you will work with images. Here is an easy way to load images off disk batch by batch, so you won't run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use tf.keras.preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "```\n",
    "\n",
    "- This code assumes that the images are stored in a directory with sub-directories for each label.\n",
    "- The output is tf.data.Dataset object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
