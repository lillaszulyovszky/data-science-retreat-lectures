{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.utils import make_writer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common bugs I\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Chapter 4 of Deep learning book. Numerical computation](https://www.deeplearningbook.org/contents/numerical.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect tensor shapes\n",
    "\n",
    "### Most common reasons:\n",
    "\n",
    "- Flipped dimensions when using tf.reshape.\n",
    "- Sum, avg, softmax over wrong dimension.\n",
    "- Forgot to flatten after conv layers.\n",
    "- Forgot to get rid of extra \"1\" dimensions, e.g. if shape is (None, 1, 1, 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In TF2, as well as in other libraries, you can accidentally broadcast tensors and then it can fail silently or just output wrong results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0.1, 0.7, 0.02, 0.08, 0.05, 0.05])\n",
    "y_true_extra_dim = np.expand_dims(y_true, -1)\n",
    "y_pred = np.array([0.1, 0.6, 0.05, 0.05, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_true: {y_true} \\n')\n",
    "print(f'Shape of y_true: {y_true.shape} \\n')\n",
    "print(f'y_true_extra_dim: {y_true_extra_dim} \\n')\n",
    "print(f'Shape of y_true_extra_dim: {y_true_extra_dim.shape} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to divide y_true by y_pred. What shapes do we expect to get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_true / y_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_true_extra_dim / y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_true_extra_dim / y_pred).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL-divergence\n",
    "\n",
    "KL-divergence is used in some models like VAEs or Bayesian models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "print(f'KLD for y_true: {kl(y_true, y_pred).numpy()} \\n')\n",
    "print(f'KLD for y_true_extra_dim: {kl(y_true_extra_dim, y_pred).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing inputs incorrectly\n",
    "\n",
    "- Forgot to standardize/scale.\n",
    "    -  It makes the resulting model dependent on the choice of units used in the input.\n",
    "- Too much augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Regression example with Auto MPG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data and create a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = tf.keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop NAs\n",
    "- Drop categorical data for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset.drop('Origin', axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dataset.describe()\n",
    "stats = stats.transpose()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.pop('MPG')\n",
    "labels = np.array(labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the difference in scales for some features even more pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Horsepower'] = dataset['Horsepower'] * 1000\n",
    "dataset['Displacement'] = dataset['Displacement'] / 1000\n",
    "train_set = np.array(dataset).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the statistics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dataset.describe()\n",
    "stats = stats.transpose()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_shape, optimizer):\n",
    "        super(RegressorNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(input_shape),\n",
    "            tf.keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
    "            tf.keras.layers.Dense(64, activation='relu', name='dense_2'),\n",
    "            tf.keras.layers.Dense(1, activation='linear', name='dense_out')\n",
    "        ])\n",
    "    \n",
    "    def summary(self):\n",
    "        self.regressor.summary()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.regressor(X)\n",
    "    \n",
    "    def get_loss(self, X, y_true):\n",
    "        y_pred = self(X)\n",
    "        l2_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        return l2_loss\n",
    "    \n",
    "    def grad_step(self, X, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(X, y_true)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model = RegressorNet(input_shape=train_set.shape[1], optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and output to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, X, y, save_dir):\n",
    "    \n",
    "    writer = make_writer(os.path.join('summaries'), save_dir)\n",
    "    \n",
    "    for epoch in range(0, epochs + 1):\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch {} is running...'.format(epoch))\n",
    "            \n",
    "        # Gradient update step\n",
    "        loss, gradients = model.grad_step(X, y.reshape(-1, 1))\n",
    "        loss = tf.math.reduce_mean(loss, axis=0)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'{loss}')\n",
    "        \n",
    "        # Tensorboard\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Train loss', loss, step=epoch)\n",
    "\n",
    "            for layer_number, layer in enumerate(model.trainable_variables):\n",
    "                tf.summary.histogram(layer.name, gradients[layer_number], step=epoch, buckets=1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, 1000, train_set, labels, 'scaling/regression_not_standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "- Write a function to standardize the data and apply it.\n",
    "- Train with the new data for 1000 epochs and send the Tensorboard output to a new directory.\n",
    "- Why does the training depend so much on the scaling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect input to the loss/ incorrect loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmaxed outputs to a loss that expects logits or vice-versa.\n",
    "- One-hot encoded labels to a sparse categorical cross-entropy loss.\n",
    "- ReLU in the last layer for regression problems.\n",
    "- E.g. MSE loss when categorical loss is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "# Add a channels dim\n",
    "x_train = tf.expand_dims(x_train[:1000].astype('float32'), axis=-1)\n",
    "y_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', name='conv_1')\n",
    "        self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu', name='dense_1')\n",
    "        self.d2 = tf.keras.layers.Dense(10, name='dense_out')\n",
    "            \n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions) \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "writer = make_writer(os.path.join('summaries'), 'loss_bug/logits_false')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        gradients = train_step(images, labels)\n",
    "\n",
    "    # Tensorboard\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Train loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('Train Accuracy', train_accuracy.result() * 100, step=epoch)\n",
    "        \n",
    "        for layer_number, layer in enumerate(model.trainable_variables):\n",
    "            tf.summary.histogram('/'.join(layer.name.split('/')[1:]), gradients[layer_number], step=epoch, buckets=1)    \n",
    "\n",
    "    message = (f'Epoch: {epoch + 1}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result() * 100}'\n",
    "              )\n",
    "    print(message)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "- Fix the loss above\n",
    "- Reinitialize the model and retrain.\n",
    "- Output to a new Tensorboard directory to compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
