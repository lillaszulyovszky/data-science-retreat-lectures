{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from utils.utils import make_writer\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common bugs II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Understanding LSTM networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Batch normalization explained](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "- [See-RNN package](https://github.com/OverLordGoldDragon/see-rnn)\n",
    "- [Gradient clipping](http://proceedings.mlr.press/v28/pascanu13.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical instabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of theory:\n",
    "\n",
    "- $X$ - input\n",
    "- $o$ - output\n",
    "- $L$ - number of layers in the network\n",
    "- $l$ - layer $l$ with the transormation $f_l$ and corresponding weights $W^l$ and the hidden variable $h^l$\n",
    "\n",
    " $$h^l = f_l(h^{l-1})$$ $$o = f_L \\circ ... \\circ f_1(X)$$\n",
    " \n",
    "If all $h^l$ and the input are vectors, one can write the gradient of $o$ with respect to any set of parameters $W^l$ as:\n",
    "\n",
    "$$\\partial_{W^l}o = \\partial_{h^{L-1}}{h^L} ... \\partial_{h^{l}}h^{l+1} \\partial_{W^{l}}h^l   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients:\n",
    "\n",
    "- Historically sigmoid was used as an activation function.\n",
    "- It resembles a thresholding function and was appealing, since neural nets were inspired by biological neural networks, where biological neurons either fire fully or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.range(-8.0, 8.0, 0.1))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.leaky_relu(x)\n",
    "\n",
    "grad = tape.gradient(y, x).numpy()\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "    \n",
    "plt.plot(x, y, 'b', label='sigmoid')\n",
    "plt.plot(x, grad, 'r', label='gradient') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradients vanish for both big and small inputs.\n",
    "- Multiply many layers and if the inputs are not near zero, then a gradient can vanish.\n",
    "- Gradient cut off at some layer --> difficult to build deep nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- What's the standard solution for this specific instance of the problem?\n",
    "- Make a similar plot for the solution.\n",
    "- What is the problem of the standard solution? Can we improve it? Please, plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients in RNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See-rnn package helps debug and analyze RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "from see_rnn import get_gradients, features_0D, features_1D, features_2D\n",
    "\n",
    "def make_model(rnn_layer, batch_shape, units):\n",
    "    ipt = tf.keras.layers.Input(batch_shape=batch_shape)\n",
    "    x   = rnn_layer(units, activation='tanh', return_sequences=True)(ipt)\n",
    "    out = rnn_layer(units, activation='tanh', return_sequences=False)(x)\n",
    "    model = tf.keras.models.Model(ipt, out)\n",
    "    model.compile(tf.keras.optimizers.Adam(4e-3), 'mse')\n",
    "    return model\n",
    "    \n",
    "def make_data(batch_shape):\n",
    "    return np.random.randn(*batch_shape), \\\n",
    "           np.random.uniform(-1, 1, (batch_shape[0], units))\n",
    "\n",
    "def train_model(model, iterations, batch_shape):\n",
    "    x, y = make_data(batch_shape)\n",
    "    for i in range(iterations):\n",
    "        model.train_on_batch(x, y)\n",
    "        print(end='.')  # progbar\n",
    "        if i % 40 == 0:\n",
    "            x, y = make_data(batch_shape)\n",
    "\n",
    "units = 16\n",
    "batch_shape = (32, 100, 2*units)\n",
    "\n",
    "model_rnn = make_model(tf.keras.layers.SimpleRNN, batch_shape, units)\n",
    "model_lstm = make_model(tf.keras.layers.LSTM, batch_shape, units)\n",
    "train_model(model_rnn, 300, batch_shape)\n",
    "train_model(model_lstm, 300, batch_shape)\n",
    "\n",
    "x, y  = make_data(batch_shape)\n",
    "grads_all_rnn  = get_gradients(model_rnn, 1, x, y)\n",
    "grads_all_lstm  = get_gradients(model_lstm, 1, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RNN VS LSTM 1D\n",
    "features_1D(grads_all_rnn[:, :, :5], n_rows=5, show_xy_ticks=[1,1])\n",
    "features_1D(grads_all_lstm[:, :, :5], n_rows=5, show_xy_ticks=[1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding gradients and gradient clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.random.normal((4, 4))\n",
    "print(f'A single matrix \\n \\n {M.numpy()}')\n",
    "for i in range(100):\n",
    "    M = tf.matmul(M, tf.random.normal((4, 4)))\n",
    "\n",
    "print(f'\\nAfter multiplying 100 matrices \\n \\n {M.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient clipping\n",
    "\n",
    "- Clip a gradient by norm:\n",
    "    - $ \\textbf{g} \\gets min\\left(1, \\frac{\\theta}{||\\textbf{g}||}\\textbf{g} \\right)$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 6]$$ $$\\theta = 5$$ $$||\\textbf{g}|| = 7$$ $$\\textbf{g} \\gets [-2, 3, 6]\\cdot \\frac{5}{7}$$\n",
    "    \n",
    "- Clip gradient by value:\n",
    "    - If $g_i < \\theta_1$, then $g_i \\gets \\theta_1$ and $g_i > \\theta_2$, then $g_i \\gets \\theta_2$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 10]$$ $$\\theta_1 = 0, \\theta_2 = 5$$  $$ \\textbf{g} \\gets [0, 3, 5]$$\n",
    "\n",
    "    \n",
    "- Clip gradient by global norm:\n",
    "    - Rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold.\n",
    "    - For example: $$\\textbf{g}_1 = [-2, 3, 6]$$ $$\\textbf{g}_2= [-4, 6, 12]$$ $$\\theta = 14$$ $$||\\textbf{g}_1|| = 7$$ $$||\\textbf{g}_2|| = 14$$ $$\\textbf{g}_1 \\gets [-2, 3, 6]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}}$$ $$\\textbf{g}_2 \\gets [-4, 6, 12]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}} $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :].astype('float32'), X[n_train:, :].astype('float32')\n",
    "trainy, testy = y[:n_train].astype('float32'), y[n_train:].astype('float32')\n",
    "\n",
    "# Creat tf.Datasets \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy)).shuffle(trainX.shape[0]).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testX, testy)).shuffle(testX.shape[0]).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNet(tf.keras.Model):\n",
    "    def __init__(self, input_shape, optimizer):\n",
    "        super(RegressorNet, self).__init__()\n",
    "        \n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = optimizer\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(input_shape),\n",
    "            tf.keras.layers.Dense(25, activation='relu', kernel_initializer='he_uniform', name='dense_1'),\n",
    "            tf.keras.layers.Dense(1, activation='linear', name='out')\n",
    "        ])\n",
    "    \n",
    "    def summary(self):\n",
    "        self.regressor.summary()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.regressor(X)\n",
    "    \n",
    "    def get_loss(self, X, y_true):\n",
    "        y_pred = self(X)\n",
    "        l2_loss = self.loss_object(y_true, y_pred)\n",
    "        return l2_loss\n",
    "    \n",
    "    def grad_step(self, X, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(X, y_true)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an optimizer and an instance of the model\n",
    "optimizer = tf.keras.optimizers.SGD(0.01, 0.9)\n",
    "model_non_clipped = RegressorNet(input_shape=trainX.shape[1], optimizer=optimizer)\n",
    "# Show summary\n",
    "model_non_clipped.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss objects for calculation of the mean loss across batches\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataset, test_dataset, save_dir):\n",
    "    \n",
    "    writer = make_writer(os.path.join('summaries'), save_dir)\n",
    "    \n",
    "    for epoch in range(0, epochs + 1):\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {} is running...'.format(epoch))\n",
    "        \n",
    "        for X, y in train_dataset:\n",
    "            # Gradient update step\n",
    "            loss_train, gradients = model.grad_step(X, y)\n",
    "            train_loss(loss_train)\n",
    "            \n",
    "        for X, y in test_dataset:\n",
    "            # Gradient update step\n",
    "            loss_test = model.get_loss(X, y)\n",
    "            test_loss(loss_test)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Train loss: {train_loss.result()}')\n",
    "\n",
    "        # Tensorboard\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Test loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('Train loss', train_loss.result(), step=epoch)\n",
    "            \n",
    "            for layer_number, layer in enumerate(model.trainable_variables):\n",
    "                tf.summary.histogram(layer.name, gradients[layer_number], step=epoch, buckets=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_non_clipped, 100, train_dataset, test_dataset, 'exploding_grads/no_clipping/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Implement gradient clipping by norm OR by value OR by global norm in a new class RegressorNetClipped.\n",
    "- Plot gradients with clipping.\n",
    "\n",
    "A threshold is a parameter. In most of the cases it's a small number, usually around 1.\n",
    "However, one can experiment with that, bigger numbers speed up learning, but too big of a threshold can make it unstable.\n",
    "Another rule of thumb to choose a threshold is to monitor an average norm of the gradients for a big number of updates, then set the threshold to 5-10 times the value of that average.\n",
    "\n",
    "Hint: check `tf.clip_by_value`, `tf.clip_by_norm`, `tf.clip_by_global_norm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common issues and causes\n",
    "\n",
    "- Too big a tensor:\n",
    "    - Too large a batch size for your model \n",
    "    - Too many fully connected layers\n",
    "- Too much data:\n",
    "    - Loading a too big dataset into memory instead of using, e.g. tf.data queue loading\n",
    "    - Allocating to large a buffer for dataset creation\n",
    "- Duplicating operations:\n",
    "    - Memory leak due to creating multiple models at the same time\n",
    "    - Repeatedly creating an operation (e.g. in a function that gets called many times)\n",
    "- Other processes:\n",
    "    - Other processes taking GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you will work with images. Here is an easy way to load images off disk batch by batch, so you won't run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use tf.keras.preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "```\n",
    "\n",
    "- This code assumes that the images are stored in a directory with sub-directories for each label.\n",
    "- The output is tf.data.Dataset object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
