{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a bug free model\n",
    "\n",
    "- Key idea: **Be suspicious and start simple** \n",
    "\n",
    "## Resources:\n",
    "\n",
    "- [Debug a deep learning network](https://medium.com/@jonathan_hui/debug-a-deep-learning-network-part-5-1123c20f960d)\n",
    "- [Troubleshooting deep learning models](https://www.youtube.com/watch?v=GwGTwPcG0YM&feature=youtu.be)\n",
    "- [Recipe for training neural networks](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Machine learning yearning](https://www.deeplearning.ai/machine-learning-yearning/)\n",
    "- [Bayesian optimization](http://krasserm.github.io/2018/03/21/bayesian-optimization/)\n",
    "- [Dive into deep learning](https://d2l.ai/index.html) \n",
    "- [Deep learning](https://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for debugging in pseudocode:\n",
    "\n",
    "1. Start simple.\n",
    "2. Implement and debug.\n",
    "3. Evaluate:\n",
    "    - if meets requirements --> Done.\n",
    "    - else:\n",
    "        - Improve model/data and go to step 2.\n",
    "        - OR tune hyperparameters and go to step 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Choose the simplest model and data possible.\n",
    "- Once it runs, overfit a single batch and reproduce a known result. If a result is not known, then try to come up with some realistic baseline, using either common sense or human level performance.\n",
    "- Apply bias-variance trade off.\n",
    "- Tune hyperparameters.\n",
    "- Bigger model if you underfit, add data or regularize if you overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Start simple \n",
    "\n",
    "### 1.1 Choose a simple architecture\n",
    "\n",
    "|  | Start |  Later |\n",
    "| ----------- | ----------- | ---------- |\n",
    "| Images | LeNet-like architecture  | ResNet, Inception |\n",
    "| Sequences | a one layer LSTM  or temporal convolutions | Attention model, Transformers |\n",
    "| Other | Fully connected with one hidden layer | Depends on the problem |\n",
    "\n",
    "Different modalities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defaults to start with\n",
    "\n",
    "- Optimizer: Adam.\n",
    "- Activations: ReLU for FC and Conv, tanh for LSTMs.\n",
    "- Initialization: \n",
    "    - Glorot Normal, Glorot Uniform (simple defaults used for the layers in TF2)\n",
    "    - Initialize in a **smart** way: if you have, for example, a regression problem with the mean of outputs equal to 50, set the bias of the last vector to 50.\n",
    "- No regularization.\n",
    "- No batch, layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF2 defaults\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "initializer = tf.keras.initializers.GlorotUniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scale/Standardize input data\n",
    "\n",
    "#### Min-max\n",
    "\n",
    "Scales to [0, 1]\n",
    "- Doesn't shift/center the data.\n",
    "- Retains sparsity.\n",
    "- Retains zero values.\n",
    "    \n",
    "\n",
    "$$ \\hat{x} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "- Use when your features are in a limited range.\n",
    "- When you do not know the distribution of your data.\n",
    "- When you know the distribution is not Gaussian.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "#### Standardization\n",
    "\n",
    "Scales the distribution to have zero mean and unit standard deviation:\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu }{\\sigma}$$\n",
    "\n",
    "- Assumes that your data has a Gaussian distribution.\n",
    "\n",
    "#### Images\n",
    "\n",
    "- TF2: divide by 255\n",
    "- PyTorch: divide by 255 and then:\n",
    "\n",
    "$$ \\hat{x} = \\frac{x - \\mu }{\\sigma} $$\n",
    "$$ \\hat{x} = \\frac{\\hat{x} - 0.5}{0.5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Simplify the problem\n",
    "\n",
    "It often makes sense to do as a starting point:\n",
    "\n",
    "- For example, reduce the training set size.\n",
    "- Use a smaller number of classes, image size, etc.\n",
    "- Create a synthetic training set that is easier to work with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implement and debug\n",
    "\n",
    "- Get your model to run.\n",
    "- Overfit a single batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 General advice for implementing your model\n",
    "\n",
    "- Minimum number of lines of code for your version 1 (rule of thumb < 200 lines, not counting already tested components).\n",
    "- Use off the shelf components:\n",
    "    - Keras for simpler tasks where no to little changes for default behaviour of functions is needed.\n",
    "    - Low level TF2, but with tf.keras.layers, tf.losses, etc. when more flexibility is needed.\n",
    "- Start with a dataset that loads into memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Overfit a single batch\n",
    "\n",
    "Assuming your model runs, try to overfit a single batch:\n",
    "\n",
    "- Error goes up:\n",
    "    - Flipped the sign of the loss/gradient.\n",
    "    - LR too high.\n",
    "    - Softmax taken over wrong dimension. \n",
    "- Error explodes:\n",
    "    - Numerical issue. Check all exp, log, div operations, clip gradients.\n",
    "    - LR too high. \n",
    "- Error oscillates:\n",
    "    - Data labels corrupted.\n",
    "    - LR too high.\n",
    "- Error plateaus:\n",
    "    - LR too low.\n",
    "    - Gradients not flowing through the whole model.\n",
    "    - Too much regularization.\n",
    "    - Incorrect input to loss function (e.g. softmax instead of logits, ReLU on output).\n",
    "    - Data labels corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Evaluate\n",
    "\n",
    "### 3.1 Compare to a known result\n",
    "\n",
    "| Usefulness in decreasing order | Source |\n",
    "| ----------- | ----------- |\n",
    "| 1 | Oficial model implementation evaluated on similar dataset |\n",
    "| 2 | Oficial model implementation evaluated on a benchmark (e.g. MNIST) |\n",
    "| 3 | Unoficial model implementation |\n",
    "| 4 | Results from a paper (with no code |\n",
    "| 5 | Results from your model on a benchmark dataset (e.g. MNIST) |\n",
    "| 6 | Compare similar model on a similar dataset |\n",
    "| 7 | Super simple baseline (avg. of all outputs, linear regression, common sense) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Perform error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carry out error analysis by manually examining ~100 val set examples the model stuggles to predict and counting the major categories of errors. Use this information to prioritize what types of errors to work on.\n",
    "- Consider splitting the val set into an Eyeball val set, which you will manually examine, and a Blackbox val set, which you will not manually examine. If performance on the Eyeball val set is much better than the Blackbox val set, you have overfit the Eyeball set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learnining curves and bias-variance trade-off:\n",
    "\n",
    "- **Unavoidable bias:** an error rate achieved for a given problem by an optimal algorithm.\n",
    "    - For example, we have a speech recognition system to train and 14% of all the audio clips are so noisy that even a human can't distinguish the words. So the best possible algorithm would have also 14% of an error rate.\n",
    "- **Avoidable bias:** the difference between the training error and the unavoidable bias. \n",
    "- **Variance:** the difference between the validation set error and the train set error.\n",
    "\n",
    "$$\\text{Bias} = \\text{Avoidable bias} + \\text{Unavoidable bias}$$\n",
    "\n",
    "There is **no unavoidable variance**, since in theory we can always add more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to follow**:\n",
    "\n",
    "1. Set a desired level of performance and if possible, find out the unavoidable bias.\n",
    "2. Plot learning curves.\n",
    "3. Identify what causes underperformance and proceed to bias/variance reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias reduction techniques:**\n",
    "\n",
    "- Increase the model size: If variance increases, use regularization.\n",
    "- Modify input features based on insights from error analysis.\n",
    "- Reduce or eliminate regularization (L2 regularization, L1 regularization, dropout, batch normalization, etc): It will increase variance too.\n",
    "- Modify model architecture so that it is more suitable for your problem.\n",
    "\n",
    "*One method that is not helpful!*\n",
    "\n",
    "- Add more training data: This technique helps with variance problems, but it usually has no significant effect on bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance reduction techniques:**\n",
    "\n",
    "- Add more training data (already :)).\n",
    "- Add regularization (L2 regularization, L1 regularization, dropout): This technique reduces variance but increases bias.\n",
    "- Add early stopping: This technique reduces variance but increases bias.\n",
    "- Feature selection to decrease number/type of input features: This technique might help with variance problems, but it might also increase bias. When your training set is small, feature selection can be very useful. In deep learning usually not needed.\n",
    "- Modify input features based on insights from error analysis: Helps with variance and bias usually.\n",
    "- Modify model architecture so that it is more suitable for your problem: This technique can affect both bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random search over grid search:\n",
    "\n",
    "<img src = \"../../assets/random_search_vs_grid.png\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "When the number of parameters increases grid search is too long to perform. Time grows exponentially with the number of parameters.\n",
    "\n",
    "- Bayesian Optimization algorithm:\n",
    "    - An alternative to random search.\n",
    "    - If too many parameters and a network is big, better use random search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
